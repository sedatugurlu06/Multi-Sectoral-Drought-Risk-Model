# ==============================================================================
# PROJECT: DROUGHT RISK THESIS - SOCIAL VULNERABILITY (OPTIMIZED)
# SCRIPT: social_model.R
# ==============================================================================

library(sf)
library(tidyverse)
library(countrycode)

# --- FORCE WORKING DIRECTORY TO SCRIPT LOCATION ---
if (rstudioapi::isAvailable()) {
  library(rstudioapi)
  setwd(dirname(getActiveDocumentContext()$path))
}
print(paste("Current Working Directory locked to:", getwd()))

# --- 0. LOAD SMART WEIGHTS (The Engine) ---
# Check if file exists
weights_file <- "../data/basin_smart_weights.csv" 
if (!file.exists(weights_file)) stop("CRITICAL: 'basin_smart_weights.csv' not found. Run the pre-processor first!")

print("Loading Basin Weights...")
basin_weights <- read_csv(weights_file, show_col_types = FALSE)

# *** NEW: REMOVE RUSSIA (RU) & TURKEY (TR) ***
# The NUTS dataset often includes Candidate countries (TR) and EFTA.
# We explicitly remove them here.
excluded_countries <- c("TR", "RU", "BY", "UA", "MD") # Turkey, Russia, Belarus, Ukraine, Moldova

print(paste("Original Basin Count:", length(unique(basin_weights$HYBAS_ID))))

basin_weights <- basin_weights %>%
  # Filter out rows where the Country ID (NUTS_0_ID) is in our exclusion list
  filter(!NUTS_0_ID %in% excluded_countries)

print(paste("Filtered Basin Count (EU Only):", length(unique(basin_weights$HYBAS_ID))))

# 1. CONFIGURATION BLOCK
CONFIG <- list(
  gender = list(
    name = "Gender Equality",
    file = "../data/cleaned_gender_final.csv",  
    join_col = "ISO_CODE",
    value_col = "Gender_Score",
    weights_col = "NUTS_0_ID", # Country Level
    weight = 0.15,
    s_curve = "decreasing",
    steepness = 5.0
  ),
  rural = list(
    name = "Rural Population",
    file = "../data/clean_rural_pop_percentage.csv",
    join_col = "geo",
    value_col = "Avg_Rural_Pop_Pct",
    weights_col = "NUTS_2_ID", # Regional Level
    weight = 0.30,
    s_curve = "increasing",
    steepness = 5.0
  ),
  arope = list(
    name = "Poverty Risk (AROPE)",
    file = "../data/clean_poverty_risk_rate.csv",
    join_col = "geo",
    value_col = "Avg_Poverty_Rate",
    weights_col = "NUTS_2_ID", # Regional Level
    weight = 0.15,
    s_curve = "increasing",
    steepness = 5.0
  ),
  dependency = list(
    name = "Social Dependency",
    file = "../data/clean_social_dependency.csv",
    join_col = "geo",
    value_col = "Avg_Dependency_Ratio",
    weights_col = "NUTS_3_ID", # Sub-regional Level
    weight = 0.15,
    s_curve = "increasing",
    steepness = 5.0
  ),
  hdi = list(
    name = "Human Development Index",
    file = "../data/clean_hdi_aquastat.csv",
    join_col = "ISO_CODE",
    value_col = "Avg_HDI_National",
    weights_col = "NUTS_0_ID", # Country Level
    weight = 0.25,
    s_curve = "decreasing",
    steepness = 5.0
  )
)

# 2. DEFINE S-CURVE FUNCTIONS
s_curve_increasing <- function(x, b, k) {
  if(is.na(b)) return(x)
  1 / (1 + (b / x)^k)
}

s_curve_decreasing <- function(x, b, k) {
  if(is.na(b)) return(x)
  ifelse(x == 0, 1, 1 / (1 + (x / b)^k))
}

normalize_data <- function(values, type, steepness) {
  midpoint <- mean(values, na.rm = TRUE)
  if(is.nan(midpoint)) return(rep(NA, length(values)))
  
  if (type == "increasing") {
    return(s_curve_increasing(values, midpoint, steepness))
  } else {
    return(s_curve_decreasing(values, midpoint, steepness))
  }
}

# 3. PROCESSING FUNCTION (Fast Join Version)
process_indicator <- function(weights_df, config_item) {
  message(paste(">>> Processing:", config_item$name, "..."))
  
  # A. Load Indicator Data
  if (!file.exists(config_item$file)) stop(paste("File not found:", config_item$file))
  df <- read_csv(config_item$file, show_col_types = FALSE)
  
  if (!config_item$join_col %in% names(df)) stop(paste("Join column", config_item$join_col, "missing"))
  if (!config_item$value_col %in% names(df)) stop(paste("Value column", config_item$value_col, "missing"))
  
  # B. Standardize Data
  data_clean <- df %>%
    select(GEO_ID = all_of(config_item$join_col), 
           RAW_VALUE = all_of(config_item$value_col)) %>%
    filter(!is.na(GEO_ID) & !is.na(RAW_VALUE)) %>%
    mutate(RAW_VALUE = as.numeric(RAW_VALUE))
  
  # C. Code Correction (ISO -> NUTS)
  # NUTS uses 'EL' for Greece, ISO often uses 'GR'. NUTS uses 'FR' for France.
  # We assume basin_smart_weights uses NUTS codes (EL, UK, etc.)
  data_clean$GEO_ID[data_clean$GEO_ID == "GR"] <- "EL"
  data_clean$GEO_ID[data_clean$GEO_ID == "GB"] <- "UK"
  
  # D. THE FAST JOIN (Weights + Data)
  # We join the Weights file (left) with Data (right) based on the specific level (NUTS_0, NUTS_2, etc.)
  
  # Dynamic join: Rename the specific weights column to "GEO_ID" to match data
  weights_active <- weights_df %>%
    rename(JOIN_KEY = all_of(config_item$weights_col))
  
  combined <- weights_active %>%
    inner_join(data_clean, by = c("JOIN_KEY" = "GEO_ID"))
  
  if(nrow(combined) == 0) warning(paste("WARNING: Zero matches found for", config_item$name, ". Check region codes!"))
  
  # E. Calculate Weighted Average for each Basin
  # Logic: Sum(Value * Weight) / Sum(Weights) 
  # Note: Weights in file are (Piece Area / Total Basin Area), so they act as percentage.
  
  basin_stats <- combined %>%
    group_by(HYBAS_ID) %>%
    summarise(
      # We calculate the weighted sum. 
      # We re-normalize by sum(weight) in case some parts of the basin have missing data.
      weighted_raw_val = sum(RAW_VALUE * weight, na.rm=TRUE) / sum(weight, na.rm=TRUE)
    ) %>%
    ungroup()
  
  # F. Apply S-Curve Normalization (on the aggregated basin value)
  basin_stats$norm_score <- normalize_data(
    basin_stats$weighted_raw_val, 
    config_item$s_curve, 
    config_item$steepness
  )
  
  # G. Format Result
  result <- basin_stats %>% select(HYBAS_ID, norm_score, weighted_raw_val)
  names(result) <- c("HYBAS_ID", paste0("VULN_", config_item$name), paste0("RAW_", config_item$name))
  
  return(result)
}

# 4. EXECUTE MODEL
# Get list of unique Basins from the weights file to start our master table
master_data <- tibble(HYBAS_ID = unique(basin_weights$HYBAS_ID))

safe_process <- function(data, cfg, weights_df) {
  res <- process_indicator(weights_df, cfg)
  left_join(data, res, by="HYBAS_ID")
}

# Run all indicators
master_data <- safe_process(master_data, CONFIG$gender, basin_weights)
master_data <- safe_process(master_data, CONFIG$rural, basin_weights)
master_data <- safe_process(master_data, CONFIG$arope, basin_weights)
master_data <- safe_process(master_data, CONFIG$dependency, basin_weights)
master_data <- safe_process(master_data, CONFIG$hdi, basin_weights)

# 5. CALCULATE FINAL INDEX
print("Calculating Composite Index...")
master_data <- master_data %>%
  mutate(
    Social_Vulnerability_Index = 
      (coalesce(`VULN_Gender Equality`, 0) * CONFIG$gender$weight) +
      (coalesce(`VULN_Rural Population`, 0) * CONFIG$rural$weight) +
      (coalesce(`VULN_Poverty Risk (AROPE)`, 0) * CONFIG$arope$weight) +
      (coalesce(`VULN_Social Dependency`, 0) * CONFIG$dependency$weight) +
      (coalesce(`VULN_Human Development Index`, 0) * CONFIG$hdi$weight)
  ) %>%
  filter(Social_Vulnerability_Index > 0) # Remove empty basins

# 6. ATTACH GEOMETRY (For Mapping)
# We only load the heavy shapefile at the very end to save time
print("Attaching Geometry for final export...")
basin_shp <- st_read("../shp/hybas_eu_lev06_v1c.shp", quiet = TRUE) %>%
  st_transform(3035)

final_map_sf <- basin_shp %>%
  inner_join(master_data, by = "HYBAS_ID") 

# 7. SAVE OUTPUT
outfile <- "../data/ready_to_map_data.rds"
print(paste("Saving data to", outfile, "..."))
saveRDS(final_map_sf, outfile)

print("DONE! The script ran using the fast weights method.")

